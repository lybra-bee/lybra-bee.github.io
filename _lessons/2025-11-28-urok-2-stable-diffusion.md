---
layout: post
title: "Урок 2. Stable Diffusion от А до Я — первое изображение за 10 минут даже для полного новичка"
date: 2025-11-28
categories: uroki
lesson_number: 2
level: Новичок
duration: 60–90 минут
goal: "Научиться запускать Stable Diffusion в Google Colab, понять, как работает диффузионная модель, и сгенерировать свои первые 10–20 ИИ-картинок"
result: "Работающий вечный генератор изображений + понимание магии под капотом"
image: /assets/images/lessons/grok_image_xi8ifbq.jpg
image_alt: "Первое изображение, созданное тобой с помощью Stable Diffusion"
tags: [stable-diffusion, генерация изображений, google-colab, diffusers, flux, нейросети, искусство]
---

{% raw %}{% include lesson-header.html %}{% endraw %}

# Урок 2. Stable Diffusion от А до Я  
Как за 10 минут получить свою первую ИИ-картинку (и почему это важнее, чем кажется)

Сегодня ты сделаешь то, что ещё в 2021 году требовало видеокарты за 2000 долларов, 50 ГБ места и трёх дней настройки.  
Сегодня всё помещается в одну кнопку в браузере.

Ты запустишь **Stable Diffusion** — самую популярную открытую модель генерации изображений 2023–2025 годов — и через 10 минут после открытия ноутбука получишь свою первую картинку, созданную исключительно твоим текстом.

И самое главное: ты поймёшь, как это вообще работает.

<br>

### Содержание урока

1. Что такое Stable Diffusion и почему это революция  
2. Как устроена диффузионная модель (по-человечески, без формул)  
3. Почему Google Colab — твой лучший друг в 2025 году  
4. Пошаговый запуск (одна кнопка или копипаст)  
5. Полный код + 4 варианта моделей на выбор  
6. Как правильно писать промпты (10 лайфхаков)  
7. Домашнее задание (обязательное и очень приятное)  
8. Что будет дальше

<br>

### 1. Что такое Stable Diffusion и почему это важно

Stable Diffusion — это нейросеть, обученная на миллиардах изображений из интернета.  
Она умеет превращать **любой текст** в **реалистичное или фантастическое изображение** за 10–60 секунд.

С августа 2022 года модель стала полностью открытой.  
С ноября 2025 года она работает даже на бесплатном Google Colab с GPU T4.

Это значит:  
- Художники теперь рисуют словами  
- Дизайнеры генерируют 100 вариантов за час  
- Школьники создают обложки для своих рассказов  
- Ты — прямо сейчас — можешь стать цифровым художником без единого урока рисования

<br>

### 2. Как это работает (очень просто)

1. Берём чистый шум (серое месиво пикселей)  
2. За 20–50 шагов постепенно «вычитаем» шум  
3. На каждом шаге нейросеть смотрит: «А похоже ли это на то, что написано в промпте?»  
4. Когда шум исчезает — остаётся твоя картинка

Всё. Никакой магии. Только математика и миллиарды примеров.

<br>

### 3. Готовый ноутбук — открой одной кнопкой

<a href="https://colab.research.google.com/drive/1T5f5iN7d2rK8v9mXz1pQb3cR4sT6uV7w?usp=sharing" target="_blank" class="btn btn-success btn-lg">
  Открыть Stable Diffusion в Google Colab (вечная ссылка)
</a>

(или скопируй код ниже в любой новый ноутбук)

<br>

### 4. Полный код урока (одна ячейка — просто нажми ▶️)

# ╔════════════════════════════════════════════════════════════════════╗
# ║   STABLE DIFFUSION В COLAB — УРОК 2 (полная версия, ноябрь 2025)   ║
# ╚════════════════════════════════════════════════════════════════════╝

# 1. Установка зависимостей (2–4 минуты один раз)
!pip install -q diffusers transformers accelerate ftfy bitsandbytes==0.43.3
!pip install -q torch==2.3.0+cu121 torchvision --extra-index-url https://download.pytorch.org/whl/cu121

import torch
from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline, FluxPipeline
from IPython.display import display, Markdown

print("Всё установлено! Выбирай модель ниже")

# ──────────────────────────────
# Вариант А — классика (самый быстрый и стабильный)
# ──────────────────────────────
pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    safety_checker=None,
    requires_safety_checker=False
).to("cuda")

# ──────────────────────────────
# Вариант Б — SDXL (максимум качества, чуть медленнее)
# Раскомментируй строки ниже и закомментируй блок А
# ──────────────────────────────
# pipe = StableDiffusionXLPipeline.from_pretrained(
#     "stabilityai/stable-diffusion-xl-base-1.0",
#     torch_dtype=torch.float16,
#     variant="fp16"
# ).to("cuda")

# ──────────────────────────────
# Вариант В — Flux.1-dev (новейшая модель, ультра-реализм)
# ──────────────────────────────
# pipe = FluxPipeline.from_pretrained(
#     "black-forest-labs/FLUX.1-dev",
#     torch_dtype=torch.bfloat16
# )
# pipe.enable_model_cpu_offload()  # экономит память

# ──────────────────────────────
# Твой промпт — меняй сколько угодно!
# ──────────────────────────────
prompt = "красивая русская девушка в киберпанк-городе ночью, неоновые вывески на кириллице, дождь, отражения в лужах, кинематографично, 8k, высокая детализация"

# Дополнительные параметры (по желанию)
negative_prompt = "размыто, уродливо, артефакты, лишние конечности, плохая анатомия"

image = pipe(
    prompt,
    negative_prompt=negative_prompt,
    num_inference_steps=30,
    guidance_scale=7.5,
    height=768,
    width=768
).images[0]

display(image)
print("Готово! Меняй prompt и запускай снова — без ограничений")
---
 Drive — он будет работать вечно.
<br>

### 5. Как писать крутые промпты — 10 лайфхаков

- Пиши на русском — модель отлично понимает  
- Добавляй стиль: «в стиле Ван Гога», «кинематографично», «фотография на iPhone 16 Pro»  
- Указывай качество: «8k», «ультра детализация», «masterpiece», «highly detailed»  
- Используй negative prompt (что НЕ должно быть): «размыто, уродливо, артефакты, лишние пальцы»  
- Указывай освещение: «мягкий свет заката», «неон ночью», «золотой час»  
- Добавляй эмоции: «грустная», «восторженная», «мечтательная», «задумчивая»  
- Указывай ракурс: «портрет крупным планом», «вид сзади», «снизу вверх»  
- Добавляй детали окружения: «в библиотеке древних книг», «на Марсе во время заката», «в кофейне утром»  
- В веб-версиях добавляй соотношение сторон: --ar 16:9, --ar 9:16, --ar 1:1  
- Сохраняй лучшие промпты в отдельный файл — через месяц у тебя будет своя личная библиотека стилей  

<br>

### Домашнее задание (обязательное и очень приятное!)

- Сгенерируй минимум 10 изображений  
- Одно из них сделай максимально безумным (пример: «кот в костюме космонавта пьёт кофе на Луне, ретро-футуризм 60-х»)  
- Три лучших сохрани в хорошем качестве  
- Выложи в сторис или пост с хештегом #МойПервыйStableDiffusion  
- По желанию — напиши в описании промпт (я подскажу, как сделать ещё круче)  

Я лично посмотрю все работы и выберу **5 самых крутых** — их авторы получат:  
- ранний доступ к закрытому уроку по обучению своих LoRA  
- упоминание в следующем уроке  
- персональный промпт от меня под твой стиль  

<br>

### Что будет дальше

- Понедельник, 1 декабря — Лайфхак: как ускорить генерацию в 5–10 раз и почти не тратить GPU  
- Пятница, 5 декабря — Урок 3: ControlNet — рисуем по своим фото, скетчам и позам  
- Декабрь — обучение своих LoRA, создание личных стилей, анимация и первые шаги в 3D  

<br>

### Заключение

Сегодня ты не просто запустил код.  
Ты прикоснулся к будущему искусства.

Сохрани этот ноутбук в свой Google Drive — он будет работать вечно.  
Через год ты откроешь свои первые картинки и улыбнёшься:  
«Вот с этого всё и началось».

До понедельника,  
твой будущий цифровой соавтор
