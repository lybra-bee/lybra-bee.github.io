#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
generate_content.py
–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å—Ç–∞—Ç—å—é (OpenRouter -> Groq fallback),
–æ–±–Ω–æ–≤–ª—è–µ—Ç data/gallery.yaml –∏ data/gallery.json,
—É–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ .md —Å—Ç–∞—Ç—å–∏ (–æ—Å—Ç–∞–≤–ª—è–µ—Ç KEEP_POSTS), –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ —Ç—Ä–æ–≥–∞–µ—Ç.
"""

import os
import sys
import time
import json
import yaml
import logging
import requests
import glob
from datetime import datetime
from slugify import slugify

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# –ü–∞–ø–∫–∏ / –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
POSTS_DIR = "content/posts"
STATIC_IMAGES_DIR = "static/images/posts"
DATA_DIR = "data"
GALLERY_YAML = os.path.join(DATA_DIR, "gallery.yaml")
GALLERY_JSON = os.path.join(DATA_DIR, "gallery.json")
KEEP_POSTS = 10  # —Å–∫–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π —Å–æ—Ö—Ä–∞–Ω—è—Ç—å

# API –∫–ª—é—á–∏ (–ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –≤ workflow / secrets)
OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY")
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")

# Ensure dirs exist
os.makedirs(POSTS_DIR, exist_ok=True)
os.makedirs(STATIC_IMAGES_DIR, exist_ok=True)
os.makedirs(DATA_DIR, exist_ok=True)

# --- Helpers ---
def safe_yaml_value(v):
    if v is None:
        return ""
    return str(v).replace("\r", " ").replace("\n", " ").strip()

def try_extract_openrouter_response(data):
    try:
        if isinstance(data, dict):
            if "choices" in data and data["choices"]:
                c = data["choices"][0]
                if isinstance(c, dict):
                    if "message" in c and isinstance(c["message"], dict) and "content" in c["message"]:
                        return c["message"]["content"]
                    if "text" in c:
                        return c["text"]
            if "output" in data and isinstance(data["output"], list) and data["output"]:
                first = data["output"][0]
                if isinstance(first, dict) and "content" in first:
                    return first["content"]
        return None
    except Exception:
        return None

def generate_with_openrouter_chat(prompt, max_tokens=800, model="gpt-4o-mini"):
    if not OPENROUTER_API_KEY:
        logging.debug("OPENROUTER_API_KEY –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
        return None
    url = "https://openrouter.ai/api/v1/chat/completions"
    headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}", "Content-Type": "application/json"}
    payload = {"model": model, "messages": [{"role": "user", "content": prompt}], "max_tokens": max_tokens}
    try:
        r = requests.post(url, headers=headers, json=payload, timeout=60)
        r.raise_for_status()
        r.encoding = "utf-8"
        data = r.json()
        text = try_extract_openrouter_response(data)
        if text:
            return text.strip()
        if isinstance(data, dict) and "message" in data:
            return str(data["message"])
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è OpenRouter(chat) failed: {e}")
    return None

def generate_with_openrouter_completions(prompt, max_tokens=1200, model="gpt-4o-mini"):
    if not OPENROUTER_API_KEY:
        return None
    url = "https://openrouter.ai/api/v1/completions"
    headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}", "Content-Type": "application/json"}
    payload = {"model": model, "input": prompt, "max_tokens": max_tokens}
    try:
        r = requests.post(url, headers=headers, json=payload, timeout=60)
        r.raise_for_status()
        r.encoding = "utf-8"
        data = r.json()
        if isinstance(data, dict):
            if "output" in data and isinstance(data["output"], list) and data["output"]:
                out = data["output"][0]
                if isinstance(out, dict) and "content" in out:
                    return out["content"]
            if "choices" in data and data["choices"]:
                c = data["choices"][0]
                if isinstance(c, dict) and "text" in c:
                    return c["text"]
        return None
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è OpenRouter(completions) failed: {e}")
    return None

def generate_with_groq_chat(prompt, max_tokens=1200, model="gpt-4o-mini"):
    if not GROQ_API_KEY:
        logging.debug("GROQ_API_KEY –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
        return None
    endpoints = [
        "https://api.groq.com/openai/v1/chat/completions",
        "https://api.groq.com/v1/chat/completions",
        "https://api.groq.com/v1/predict",
    ]
    headers = {"Authorization": f"Bearer {GROQ_API_KEY}", "Content-Type": "application/json"}
    for url in endpoints:
        try:
            payload = {"model": model, "messages": [{"role": "user", "content": prompt}], "max_tokens": max_tokens}
            r = requests.post(url, headers=headers, json=payload, timeout=60)
            r.raise_for_status()
            r.encoding = "utf-8"
            data = r.json()
            if isinstance(data, dict):
                if "choices" in data and data["choices"]:
                    c = data["choices"][0]
                    if isinstance(c, dict):
                        if "message" in c and "content" in c["message"]:
                            return c["message"]["content"].strip()
                        if "text" in c:
                            return c["text"].strip()
                if "prediction" in data:
                    return str(data["prediction"]).strip()
                if "output" in data and isinstance(data["output"], list) and data["output"]:
                    first = data["output"][0]
                    if isinstance(first, dict) and "content" in first:
                        return first["content"].strip()
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Groq endpoint {url} failed: {e}")
            continue
    return None

# --- High level generators ---
def generate_title(year):
    prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–µ –∏ –≤—ã—Å–æ–∫–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö. 
–°–æ—Å—Ç–∞–≤—å –∫–æ—Ä–æ—Ç–∫–∏–π (5‚Äì9 —Å–ª–æ–≤), —ë–º–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ —Ä—É—Å—Å–∫–æ–º, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –∏–Ω—Ç–µ—Ä–µ—Å–µ–Ω –¥–ª—è –±–ª–æ–≥–∞. 
–ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–ª–∂–µ–Ω –æ—Ç—Ä–∞–∂–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π, –∫–µ–π—Å, –Ω–æ–≤–∏–Ω–∫—É, —É—Ä–æ–∫ –∏–ª–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, 
–Ω–∞–ø—Ä–∏–º–µ—Ä: ¬´–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ò–ò –≤ –º–µ–¥–∏—Ü–∏–Ω–µ¬ª, ¬´–ö–∞–∫ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å¬ª, ¬´–ò–ò –≤ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–µ¬ª, 
¬´–ù–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –¥–ª—è –º—É–∑—ã–∫–∏¬ª. –ò–∑–±–µ–≥–∞–π —Å–ª–æ–≤ ¬´—Ç—Ä–µ–Ω–¥—ã¬ª, ¬´–Ω–æ–≤–æ—Å—Ç–∏¬ª –∏ –æ–±—â–∏—Ö —Ñ—Ä–∞–∑. 
–ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º –∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã–º, –∞–∫—Ç—É–∞–ª—å–Ω—ã–º –¥–ª—è {year} –≥–æ–¥–∞.
"""
    logging.info("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ (OpenRouter -> Groq)...")
    text = generate_with_openrouter_chat(prompt, max_tokens=70)
    if text:
        logging.info("‚úÖ –ó–∞–≥–æ–ª–æ–≤–æ–∫ —á–µ—Ä–µ–∑ OpenRouter")
        return text.strip().strip('"')
    text = generate_with_openrouter_completions(prompt, max_tokens=70)
    if text:
        logging.info("‚úÖ –ó–∞–≥–æ–ª–æ–≤–æ–∫ —á–µ—Ä–µ–∑ OpenRouter (completions)")
        return text.strip().strip('"')
    text = generate_with_groq_chat(prompt, max_tokens=70)
    if text:
        logging.info("‚úÖ –ó–∞–≥–æ–ª–æ–≤–æ–∫ —á–µ—Ä–µ–∑ Groq")
        return text.strip().strip('"')
    logging.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç API ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫")
    return f"–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ò–ò –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π {year}"

def generate_article_text(title, year):
    prompt = f"–ù–∞–ø–∏—à–∏ —Å—Ç–∞—Ç—å—é –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É: ¬´{title}¬ª. –û–±—ä—ë–º 400-600 —Å–ª–æ–≤. –°–¥–µ–ª–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ –∏ —Å–≤—è–∑–Ω–æ, —Ä–∞–∑–¥–µ–ª–∏ —Ç–µ–∫—Å—Ç –Ω–∞ –≤–≤–µ–¥–µ–Ω–∏–µ, –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–¥–µ–ª–æ–≤ —Å –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –∏ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ."
    logging.info("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏ (OpenRouter -> Groq)...")
    text = generate_with_openrouter_chat(prompt, max_tokens=1100)
    if text:
        logging.info("‚úÖ –¢–µ–∫—Å—Ç –ø–æ–ª—É—á–µ–Ω —á–µ—Ä–µ–∑ OpenRouter")
        return text
    text = generate_with_openrouter_completions(prompt, max_tokens=1100)
    if text:
        logging.info("‚úÖ –¢–µ–∫—Å—Ç –ø–æ–ª—É—á–µ–Ω —á–µ—Ä–µ–∑ OpenRouter (completions)")
        return text
    text = generate_with_groq_chat(prompt, max_tokens=1100)
    if text:
        logging.info("‚úÖ –¢–µ–∫—Å—Ç –ø–æ–ª—É—á–µ–Ω —á–µ—Ä–µ–∑ Groq")
        return text
    logging.warning("‚ö†Ô∏è –í—Å–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏ ‚Äî –∏—Å–ø–æ–ª—å–∑—É—é –ª–æ–∫–∞–ª—å–Ω—ã–π fallback")
    return (
        f"## –í–≤–µ–¥–µ–Ω–∏–µ\n\n–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –≤ {year} –≥–æ–¥—É –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ —Ä–∞–∑–≤–∏–≤–∞—Ç—å—Å—è. "
        "–ù–∏–∂–µ ‚Äî —Å–≤–æ–¥–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π.\n\n"
        "## –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è\n\n- –ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ä–∞–∑–≤–∏–≤–∞—é—Ç—Å—è –∏ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –¥–æ—Å—Ç—É–ø–Ω–µ–µ.\n"
        "- –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—Ç —Ç–µ–∫—Å—Ç, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –∑–≤—É–∫.\n"
        "- –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ò–ò –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ—Ç—Ä–∞—Å–ª—è—Ö —Ä–∞—Å—à–∏—Ä—è–µ—Ç—Å—è.\n\n"
        "## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n\n–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç—Ä–∞—Å–ª–∏ –∏ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—É—é –∂–∏–∑–Ω—å."
    )

# --- Save article (front matter YAML) ---
def save_article(title, text, model_name, slug):
    fm = {
        "title": safe_yaml_value(title),
        "date": datetime.now().astimezone().isoformat(),
        "draft": False,
        "image": "",  # –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —É–∂–µ –µ—Å—Ç—å –≤ static, –Ω–µ —Å–æ–∑–¥–∞—ë–º svg
        "model": safe_yaml_value(model_name),
        "tags": ["AI", "Tech"],
        "categories": ["–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"],
        "author": "AI Generator",
        "description": safe_yaml_value(text[:200] + ("..." if len(text) > 200 else "")),
    }
    filename = os.path.join(POSTS_DIR, f"{slug}.md")
    yaml_block = yaml.safe_dump(fm, allow_unicode=True, default_flow_style=False, sort_keys=False)
    content = f"---\n{yaml_block}---\n\n{text}\n"
    try:
        with open(filename, "w", encoding="utf-8") as f:
            f.write(content)
        logging.info(f"‚úÖ Article saved: {filename}")
        return filename
    except Exception as e:
        logging.error(f"‚ùå Error saving article: {e}")
        raise

# --- Gallery generation from static/images/posts/ ---
def rebuild_gallery_from_static(limit=200):
    logging.info("üóÇÔ∏è Rebuilding gallery from static/images/posts/ ...")
    patterns = ["*.png", "*.jpg", "*.jpeg", "*.svg", "*.webp", "*.gif"]
    files = []
    for p in patterns:
        files.extend(glob.glob(os.path.join(STATIC_IMAGES_DIR, p)))
    files = sorted(files, key=os.path.getmtime, reverse=True)
    images = []
    for fpath in files[:limit]:
        fname = os.path.basename(fpath)
        title = os.path.splitext(fname)[0].replace("-", " ").replace("_", " ").title()
        item = {
            "src": f"/images/posts/{fname}",
            "alt": title,
            "title": title,
            "date": datetime.fromtimestamp(os.path.getmtime(fpath)).strftime("%Y-%m-%d")
        }
        images.append(item)
    try:
        with open(GALLERY_YAML, "w", encoding="utf-8") as yf:
            yaml.safe_dump(images, yf, allow_unicode=True, default_flow_style=False)
        with open(GALLERY_JSON, "w", encoding="utf-8") as jf:
            json.dump(images, jf, ensure_ascii=False, indent=2)
        logging.info(f"‚úÖ Gallery rebuilt: {len(images)} items ({GALLERY_YAML}, {GALLERY_JSON})")
    except Exception as e:
        logging.error(f"‚ùå Error writing gallery files: {e}")

# --- Cleanup old posts (keep only KEEP_POSTS newest) ---
def cleanup_old_posts(keep=KEEP_POSTS):
    try:
        md_files = glob.glob(os.path.join(POSTS_DIR, "*.md"))
        md_files = sorted(md_files, key=os.path.getmtime, reverse=True)
        if len(md_files) <= keep:
            logging.info("üßæ –ù–µ—Ç —Å—Ç–∞—Ä—ã—Ö —Å—Ç–∞—Ç–µ–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è")
            return
        for old in md_files[keep:]:
            try:
                os.remove(old)
                logging.info(f"üóëÔ∏è Removed old article: {old}")
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {old}: {e}")
    except Exception as e:
        logging.error(f"‚ùå Cleanup posts failed: {e}")

# --- Main ---
def main():
    year = datetime.now().year
    cleanup_old_posts()
    rebuild_gallery_from_static()
    title = generate_title(year)
    slug = slugify(title)
    article_text = generate_article_text(title, year)
    save_article(title, article_text, model_name="AI Generator", slug=slug)
    logging.info("üéâ Content generation complete.")

if __name__ == "__main__":
    main()
