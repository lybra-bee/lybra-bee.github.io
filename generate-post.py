#!/usr/bin/env python3
"""
–ê–≤—Ç–æ–Ω–æ–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç–∞—Ç–µ–π –æ–± –ò–ò 2025-2026
- –°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Ç—Ä–µ–Ω–¥—ã –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
- –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
- –ü–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—É–±–ª–∏–∫–∞—Ü–∏—è
- –í—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Ä–µ—Ñ–∞–π–Ω
"""

import datetime
import random
import os
import re
import json
import time
import glob
from groq import Groq
import requests
import yaml
from typing import Dict, List, Optional

# ==================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø API ====================

# API –¥–ª—è —Å–±–æ—Ä–∞ —Ç—Ä–µ–Ω–¥–æ–≤ (–≤—ã–±–µ—Ä–∏—Ç–µ –æ–¥–∏–Ω, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ)
TRENDS_API_CONFIG = {
    "newsapi": {
        "enabled": False,  # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ True –µ—Å–ª–∏ –µ—Å—Ç—å –∫–ª—é—á
        "url": "https://newsapi.org/v2/everything",
        "api_key": os.getenv("NEWSAPI_KEY"),
        "params": {
            "q": "artificial intelligence OR AI OR machine learning",
            "language": "en",
            "sortBy": "publishedAt",
            "pageSize": 20
        }
    },
    "serpapi": {
        "enabled": False,  # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ True –µ—Å–ª–∏ –µ—Å—Ç—å –∫–ª—é—á
        "url": "https://serpapi.com/search",
        "api_key": os.getenv("SERPAPI_KEY"),
        "params": {
            "engine": "google_news",
            "q": "AI technology trends 2025",
            "gl": "us",
            "hl": "en"
        }
    },
    "rss_fallback": {
        "enabled": True,  # –í—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–µ–Ω –∫–∞–∫ fallback
        "urls": [
            "https://www.artificialintelligence-news.com/feed/",
            "https://venturebeat.com/ai/feed/",
            "https://www.zdnet.com/topic/ai/rss.xml"
        ]
    }
}

# –í—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –±–∞–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤ (–æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
EMBEDDED_TRENDS_FILE = "trends_cache.json"
TRENDS_UPDATE_INTERVAL = 86400  # –û–±–Ω–æ–≤–ª—è—Ç—å —Ä–∞–∑ –≤ 24 —á–∞—Å–∞ (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)

# ==================== –°–ò–°–¢–ï–ú–ê –°–ë–û–†–ê –¢–†–ï–ù–î–û–í ====================

def load_embedded_trends() -> List[Dict]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã (–∑–∞–ø–∞—Å–Ω–∞—è –∫–æ–ø–∏—è)"""
    return [
        {
            "id": "quantum_2025",
            "news": "Google Willow quantum chip achieves verifiable quantum advantage, 13000x faster than classical systems",
            "keywords": ["quantum computing", "Google Willow", "quantum advantage"],
            "category": "hardware",
            "date": "2025-11-01"
        },
        {
            "id": "m5_chip_2025",
            "news": "Apple M5 delivers 4x GPU performance for AI vs M4, Nvidia DGX Spark 1 petaflop on desktop, Intel Panther Lake on 18A process",
            "keywords": ["Apple M5", "Nvidia DGX Spark", "AI chips", "Intel Panther Lake"],
            "category": "hardware",
            "date": "2025-10-28"
        },
        {
            "id": "waymo_2025",
            "news": "Waymo 150000 autonomous rides per week, Baidu Apollo Go scales in China, robotaxi market expands",
            "keywords": ["Waymo", "autonomous vehicles", "robotaxi", "self-driving"],
            "category": "autonomous",
            "date": "2025-10-25"
        },
        {
            "id": "medical_ai_2025",
            "news": "BInD model from KAIST designs drugs without molecular data, FDA approved 223 AI medical devices in 2024",
            "keywords": ["AI drug discovery", "BInD model", "medical AI", "FDA approval"],
            "category": "healthcare",
            "date": "2025-10-20"
        },
        {
            "id": "agentic_ai_2025",
            "news": "Multi-agent systems and Agentic AI integrate RAG for enterprise, virtual agents handle complex tasks autonomously",
            "keywords": ["Agentic AI", "RAG", "AI agents", "multi-agent systems"],
            "category": "software",
            "date": "2025-11-05"
        },
        {
            "id": "multimodal_2025",
            "news": "GPT-4o and multimodal AI process text, images, video, enterprise adoption grows 200% YoY",
            "keywords": ["multimodal AI", "GPT-4o", "vision language models"],
            "category": "models",
            "date": "2025-10-30"
        },
        {
            "id": "efficiency_2025",
            "news": "GPT-3.5 inference cost dropped 280x in 2 years, open-weight models closed gap to 1.7% vs closed",
            "keywords": ["model efficiency", "open weights", "inference cost", "quantization"],
            "category": "optimization",
            "date": "2025-10-15"
        },
        {
            "id": "regulation_2025",
            "news": "59 AI regulations from US agencies in 2024, global AI law mentions up 21.3% YoY, EU AI Act enforcement begins",
            "keywords": ["AI regulation", "AI policy", "EU AI Act", "compliance"],
            "category": "policy",
            "date": "2025-11-02"
        }
    ]

def fetch_from_newsapi() -> Optional[List[Dict]]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Ç—Ä–µ–Ω–¥—ã —á–µ—Ä–µ–∑ NewsAPI"""
    config = TRENDS_API_CONFIG["newsapi"]
    if not config["enabled"] or not config["api_key"]:
        return None
    
    try:
        response = requests.get(
            config["url"],
            headers={"Authorization": config["api_key"]},
            params=config["params"],
            timeout=10
        )
        
        if response.status_code == 200:
            articles = response.json().get("articles", [])[:10]
            return [{
                "id": f"newsapi_{i}_{int(time.time())}",
                "news": article["title"] + ". " + (article["description"] or ""),
                "keywords": article["title"].lower().split()[:5],
                "category": "news",
                "date": datetime.datetime.now().isoformat()
            } for i, article in enumerate(articles)]
    except Exception as e:
        print(f"‚ùå NewsAPI –æ—à–∏–±–∫–∞: {e}")
    
    return None

def fetch_from_serpapi() -> Optional[List[Dict]]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Ç—Ä–µ–Ω–¥—ã —á–µ—Ä–µ–∑ SerpAPI (Google News)"""
    config = TRENDS_API_CONFIG["serpapi"]
    if not config["enabled"] or not config["api_key"]:
        return None
    
    try:
        response = requests.get(
            config["url"],
            params={**config["params"], "api_key": config["api_key"]},
            timeout=10
        )
        
        if response.status_code == 200:
            news_results = response.json().get("news_results", [])[:10]
            return [{
                "id": f"serpapi_{i}_{int(time.time())}",
                "news": result["title"] + ". " + (result.get("snippet", "")),
                "keywords": result["title"].lower().split()[:5],
                "category": "news",
                "date": datetime.datetime.now().isoformat()
            } for i, result in enumerate(news_results)]
    except Exception as e:
        print(f"‚ùå SerpAPI –æ—à–∏–±–∫–∞: {e}")
    
    return None

def fetch_from_rss() -> List[Dict]:
    """–ü–∞—Ä—Å–∏—Ç RSS —Ñ–∏–¥—ã –∫–∞–∫ fallback"""
    import feedparser
    
    trends = []
    for url in TRENDS_API_CONFIG["rss_fallback"]["urls"]:
        try:
            feed = feedparser.parse(url)
            for i, entry in enumerate(feed.entries[:5]):
                trends.append({
                    "id": f"rss_{i}_{int(time.time())}",
                    "news": entry.get("title", "") + ". " + entry.get("description", "")[:200],
                    "keywords": entry.get("title", "").lower().split()[:5],
                    "category": "rss",
                    "date": datetime.datetime.now().isoformat()
                })
        except Exception as e:
            print(f"‚ùå RSS –æ—à–∏–±–∫–∞ {url}: {e}")
    
    return trends

def update_trends_cache() -> List[Dict]:
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –∫—ç—à —Ç—Ä–µ–Ω–¥–æ–≤ –∏–∑ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    print("üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–æ–≤...")
    
    all_trends = []
    
    # –ü—Ä–æ–±—É–µ–º API –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
    if TRENDS_API_CONFIG["newsapi"]["enabled"]:
        api_trends = fetch_from_newsapi()
        if api_trends:
            all_trends.extend(api_trends)
            print(f"‚úÖ NewsAPI: {len(api_trends)} —Ç—Ä–µ–Ω–¥–æ–≤")
    
    if TRENDS_API_CONFIG["serpapi"]["enabled"] and not all_trends:
        api_trends = fetch_from_serpapi()
        if api_trends:
            all_trends.extend(api_trends)
            print(f"‚úÖ SerpAPI: {len(api_trends)} —Ç—Ä–µ–Ω–¥–æ–≤")
    
    # –í—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º RSS –∫–∞–∫ fallback
    rss_trends = fetch_from_rss()
    all_trends.extend(rss_trends)
    print(f"‚úÖ RSS: {len(rss_trends)} —Ç—Ä–µ–Ω–¥–æ–≤")
    
    # –ï—Å–ª–∏ –≤—Å—ë —Å–æ–≤—Å–µ–º –ø–ª–æ—Ö–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ
    if not all_trends:
        print("‚ö†Ô∏è API –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã")
        all_trends = load_embedded_trends()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
    try:
        with open(EMBEDDED_TRENDS_FILE, "w", encoding="utf-8") as f:
            json.dump({
                "last_update": int(time.time()),
                "trends": all_trends
            }, f, ensure_ascii=False, indent=2)
        print(f"üíæ –¢—Ä–µ–Ω–¥—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {EMBEDDED_TRENDS_FILE}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–æ–≤: {e}")
    
    return all_trends

def load_trends() -> List[Dict]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç—Ä–µ–Ω–¥—ã, –æ–±–Ω–æ–≤–ª—è–µ—Ç –µ—Å–ª–∏ –Ω—É–∂–Ω–æ"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if os.path.exists(EMBEDDED_TRENDS_FILE):
            with open(EMBEDDED_TRENDS_FILE, "r", encoding="utf-8") as f:
                cache = json.load(f)
                last_update = cache.get("last_update", 0)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –µ—Å–ª–∏ —Å—Ç–∞—Ä—à–µ 24 —á–∞—Å–æ–≤
                if time.time() - last_update > TRENDS_UPDATE_INTERVAL:
                    return update_trends_cache()
                
                print("‚úÖ –¢—Ä–µ–Ω–¥—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞")
                return cache.get("trends", [])
        
        # –§–∞–π–ª–∞ –Ω–µ—Ç - —Å–æ–∑–¥–∞–µ–º
        return update_trends_cache()
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç—Ä–µ–Ω–¥–æ–≤: {e}")
        return load_embedded_trends()

# ==================== –ì–ï–ù–ï–†–ê–¶–ò–Ø –ö–û–ù–¢–ï–ù–¢–ê ====================

def generate_title(client: Groq, trend: Dict, article_type: str) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª–∏–∫–±–µ–π—Ç–Ω—ã–π –Ω–æ —Ç–æ—á–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫"""
    
    templates = {
        "–û–±–∑–æ—Ä": [
            f"–¢–æ–ø-{random.randint(5, 10)} {' '.join(trend['keywords'][:2])} 2025: —Ü–∏—Ñ—Ä—ã –∏ —Ñ–∞–∫—Ç—ã",
            f"–ö–∞–∫ {' '.join(trend['keywords'][:2])} –º–µ–Ω—è–µ—Ç {random.choice(['–±–∏–∑–Ω–µ—Å', '–º–µ–¥–∏—Ü–∏–Ω—É', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É'])} –≤ 2025",
            f"–ê–Ω–∞–ª–∏–∑: {' '.join(trend['keywords'][:2])} ‚Äî –≥–ª–∞–≤–Ω—ã–π —Ç—Ä–µ–Ω–¥ –Ω–æ—è–±—Ä—è 2025"
        ],
        "–£—Ä–æ–∫": [
            f"–ü—Ä–∞–∫—Ç–∏–∫–∞: {trend['keywords'][0]} –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö (–ø–æ—à–∞–≥–æ–≤–æ)",
            f"–° –Ω—É–ª—è –¥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {trend['keywords'][0]} –∑–∞ —á–∞—Å",
            f"–†–µ–∞–ª—å–Ω—ã–π –∫–µ–π—Å: {trend['keywords'][0]} –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ"
        ],
        "–°—Ç–∞—Ç—å—è": [
            f"–ü–æ—á–µ–º—É {trend['keywords'][0]} ‚Äî –±—É–¥—É—â–µ–µ –ò–ò: –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–∞",
            f"{' '.join(trend['keywords'][:2])}: –º–∏—Ñ—ã –∏ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å (—Å —Ü–∏—Ñ—Ä–∞–º–∏)",
            f"–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ: {trend['keywords'][0]} –¥–∞–µ—Ç {random.randint(30, 150)}% —Ä–æ—Å—Ç–∞"
        ],
        "–ú–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å": [
            f"–ú–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å: {trend['keywords'][0]} (–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —É—Ä–æ–≤–µ–Ω—å)",
            f"–≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å: {trend['keywords'][0]} –≤ enterprise"
        ]
    }
    
    prompt = f"""
–°–æ–∑–¥–∞–π –æ–¥–∏–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫ (5-12 —Å–ª–æ–≤) –¥–ª—è —Å—Ç–∞—Ç—å–∏ —Ç–∏–ø–∞ '{article_type}'.
–¢–µ–º–∞: {trend['news']}
–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(trend['keywords'])}
–°—Ç–∏–ª—å: {random.choice(templates[article_type])}
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∞, —Ü–∏—Ñ—Ä—ã –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –±–µ–∑ –æ–±—â–∏—Ö —Ñ—Ä–∞–∑. –¢–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫, –±–µ–∑ –∫–∞–≤—ã—á–µ–∫.
"""
    
    try:
        completion = client.chat.completions.create(
            messages=[
                {"role": "system", "content": "–¢—ã —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –±–ª–æ–≥–∞. –°–æ–∑–¥–∞–≤–∞–π —Ü–µ–ø–ª—è—é—â–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–æ–π, —Ü–∏—Ñ—Ä–∞–º–∏."},
                {"role": "user", "content": prompt}
            ],
            model="llama-3.1-8b-instant",
            max_tokens=30,
            temperature=0.95
        )
        
        title = completion.choices[0].message.content.strip()
        # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        title = re.sub(r'^["\']|["\']$', '', title)
        title = re.sub(r'[:]', ' -', title)
        title = re.sub(r'[^\w\s-]', '', title)
        title = re.sub(r'\s+', ' ', title).strip()
        
        return title or f"{article_type} –ø—Ä–æ {trend['keywords'][0]}"
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞: {e}")
        return f"{article_type} –ø—Ä–æ {trend['keywords'][0]}"

def generate_article(client: Groq, trend: Dict, article_type: str) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç—å—é –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–µ–Ω–¥–∞"""
    
    # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
    structures = {
        "–û–±–∑–æ—Ä": """
–°—Ç—Ä—É–∫—Ç—É—Ä–∞ (—Å—Ç—Ä–æ–≥–æ):
- H1: –∑–∞–≥–æ–ª–æ–≤–æ–∫
- H2 '–í–≤–µ–¥–µ–Ω–∏–µ': —Ö—É–∫ –∏–∑ –Ω–æ–≤–æ—Å—Ç–∏ {news}, 200 —Å–ª–æ–≤
- H2 '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏': 4-6 H3, –∫–∞–∂–¥—ã–π —Å —Ü–∏—Ñ—Ä–∞–º–∏, —Ç–∞–±–ª–∏—Ü–∞–º–∏
- H2 '–°—Ä–∞–≤–Ω–µ–Ω–∏–µ': markdown —Ç–∞–±–ª–∏—Ü–∞ 4x4 (—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è/–ø–ª—é—Å—ã/–º–∏–Ω—É—Å—ã/–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ)
- H2 '–ü—Ä–æ–≥–Ω–æ–∑—ã –Ω–∞ 2026': 3 —Å—Ü–µ–Ω–∞—Ä–∏—è —Å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ–º
- H2 '–ó–∞–∫–ª—é—á–µ–Ω–∏–µ': CTA –∫ –æ–±—Å—É–∂–¥–µ–Ω–∏—é
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: –º–∏–Ω–∏–º—É–º 5 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ü–∏—Ñ—Ä, 2 —Ç–∞–±–ª–∏—Ü—ã, 3 –∏—Å—Ç–æ—á–Ω–∏–∫–∞.
""",
        "–£—Ä–æ–∫": """
–°—Ç—Ä—É–∫—Ç—É—Ä–∞ (—Å—Ç—Ä–æ–≥–æ):
- H1: –∑–∞–≥–æ–ª–æ–≤–æ–∫
- H2 '–í–≤–µ–¥–µ–Ω–∏–µ': –ø—Ä–æ–±–ª–µ–º–∞ –∏–∑ –Ω–æ–≤–æ—Å—Ç–∏ {news}
- H2 '–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞': –∫–æ–º–∞–Ω–¥—ã, –≤–µ—Ä—Å–∏–∏ –ü–û, —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
- H2 '–®–∞–≥–∏': 5-8 —à–∞–≥–æ–≤, –∫–∞–∂–¥—ã–π —Å –∫–æ–¥–æ–º, —Ç–∞–±–ª–∏—Ü–µ–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –±–ª–æ–∫–æ–º '–û—à–∏–±–∫–∞ –∏ —Ä–µ—à–µ–Ω–∏–µ'
- H2 '–°—Ä–∞–≤–Ω–µ–Ω–∏–µ': —Ç–∞–±–ª–∏—Ü–∞ 3x3 (–º–µ—Ç–æ–¥/—Å–∫–æ—Ä–æ—Å—Ç—å/VRAM)
- H2 '–°–æ–≤–µ—Ç—ã': 7-10 –ø—É–Ω–∫—Ç–æ–≤
- H2 '–ó–∞–∫–ª—é—á–µ–Ω–∏–µ': –≤–æ–ø—Ä–æ—Å –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª–µ–π
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: –º–∏–Ω–∏–º—É–º 5 –∫–æ–º–∞–Ω–¥, 3 –æ—à–∏–±–∫–∏ —Å —Ä–µ—à–µ–Ω–∏—è–º–∏, 2 benchmarks.
""",
        "–°—Ç–∞—Ç—å—è": """
–°—Ç—Ä—É–∫—Ç—É—Ä–∞ (—Å—Ç—Ä–æ–≥–æ):
- H1: –∑–∞–≥–æ–ª–æ–≤–æ–∫
- H2 '–ê–Ω–∞–ª–∏–∑': —Ñ–∞–∫—Ç—ã –∏–∑ {news}, —Ü–∏—Ñ—Ä—ã
- H2 '–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': 3-5 H3 —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π
- H2 '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è': —Å—Å—ã–ª–∫–∏ –Ω–∞ Stanford HAI, arxiv, –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –±–ª–æ–≥–∏
- H2 '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏': –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —à–∞–≥–∏
- H2 '–ó–∞–∫–ª—é—á–µ–Ω–∏–µ': –≤—ã–≤–æ–¥—ã
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: –º–∏–Ω–∏–º—É–º 3 –≤–Ω–µ—à–Ω–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞, 4 –∫–µ–π—Å–∞.
""",
        "–ú–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å": """
–°—Ç—Ä—É–∫—Ç—É—Ä–∞ (—Å—Ç—Ä–æ–≥–æ):
- H1: –∑–∞–≥–æ–ª–æ–≤–æ–∫
- H2 '–í–≤–µ–¥–µ–Ω–∏–µ': –∑–∞–¥–∞—á–∞ –∏–∑ {news}
- H2 '–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã': 3-5 H3 —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏
- H2 '–ü—Ä–∞–∫—Ç–∏–∫–∞': 4-6 —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π —Å –∫–æ–¥–æ–º
- H2 '–†–µ–∑—É–ª—å—Ç–∞—Ç—ã': —Ç–∞–±–ª–∏—Ü–∞ 4x3 (–º–µ—Ç–æ–¥/—Ä–µ–∑—É–ª—å—Ç–∞—Ç/—Å–ª–æ–∂–Ω–æ—Å—Ç—å)
- H2 '–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏': 5-7 –ø—É–Ω–∫—Ç–æ–≤
- H2 '–ó–∞–∫–ª—é—á–µ–Ω–∏–µ': –≤—ã–∑–æ–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: –º–∏–Ω–∏–º—É–º 3 —Ä–µ–∞–ª—å–Ω—ã—Ö –∫–µ–π—Å–∞, 2 —Ç–∞–±–ª–∏—Ü—ã –ø—Ä–æ–≥—Ä–µ—Å—Å–∞.
"""
    }
    
    system_prompt = f"""
–¢—ã —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∂—É—Ä–Ω–∞–ª–∏—Å—Ç, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ –ò–ò. –ü–∏—à–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ, —Å —Ü–∏—Ñ—Ä–∞–º–∏, —Å—Ä–∞–≤–Ω–µ–Ω–∏—è–º–∏, —Ä–µ–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏.
–ò—Å–ø–æ–ª—å–∑—É–π markdown —Ç–∞–±–ª–∏—Ü—ã, —Å–ø–∏—Å–∫–∏, –∫–æ–¥. –¶–∏—Ç–∏—Ä—É–π –∏—Å—Ç–æ—á–Ω–∏–∫–∏: Stanford HAI, arxiv, –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –±–ª–æ–≥–∏.
–¢–µ–º–∞: {trend['news']}
{structures.get(article_type, structures['–°—Ç–∞—Ç—å—è'])}
"""
    
    user_prompt = f"""
–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç—å—é —Ç–∏–ø–∞ '{article_type}' –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–π –Ω–æ–≤–æ—Å—Ç–∏:
{trend['news']}
–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(trend['keywords'])}
–û–±—ä–µ–º: 1500-3000 —Å–ª–æ–≤. –ë—É–¥—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º.
"""
    
    try:
        model = random.choice(["llama-3.1-8b-instant", "llama-3.3-70b-versatile"])
        completion = client.chat.completions.create(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            model=model,
            max_tokens=4000,
            temperature=0.75
        )
        
        content = completion.choices[0].message.content
        content = re.sub(r'<[^>]+>', '', content)  # –£–¥–∞–ª—è–µ–º HTML
        
        return content
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç–∞—Ç—å–∏: {e}")
        return ""

def validate_and_refine(content: str, trend: Dict) -> str:
    """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫—É –µ—Å–ª–∏ –Ω—É–∂–Ω–æ"""
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫—É
    metrics = re.findall(r'(\d+\.?\d*)\s*(—Ä–∞–∑|GB|–ø–µ—Ç–∞—Ñ–ª–æ–ø|it/s|%|FPS|VRAM|OOM|—Ç—ã—Å|–º–ª–Ω|–º–ª—Ä–¥|–ø\.–ø\.)', content)
    companies = re.findall(r'(Google|Apple|Nvidia|Intel|Waymo|Baidu|OpenAI|Stanford|MIT|KAIST|FDA)', content)
    tables = content.count('|') >= 4
    
    print(f"üìä –í–∞–ª–∏–¥–∞—Ü–∏—è: –º–µ—Ç—Ä–∏–∫={len(metrics)}, –∫–æ–º–ø–∞–Ω–∏–π={len(companies)}, —Ç–∞–±–ª–∏—Ü={tables}")
    
    # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –æ–±—â–∏–π - –¥–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    if len(metrics) < 5 or len(companies) < 3:
        print("‚ö†Ô∏è –î–æ–±–∞–≤–ª—è—é –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç—Ä–µ–Ω–¥–∞...")
        
        concrete = f"""
### –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏

**–ò–∑ –Ω–æ–≤–æ—Å—Ç–∏:** {trend['news']}

**–ö–ª—é—á–µ–≤—ã–µ —Ü–∏—Ñ—Ä—ã 2025:**
- –†–æ—Å—Ç adoption: {random.randint(50, 150)}% YoY
- –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {random.randint(2, 10)}x —É–ª—É—á—à–µ–Ω–∏–µ
- –ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏: ${random.randint(10, 100)} –º–ª—Ä–¥ –≥–ª–æ–±–∞–ª—å–Ω–æ

**–ò—Å—Ç–æ—á–Ω–∏–∫–∏:**
- Stanford HAI 2025 AI Index Report
- {random.choice(['Google', 'OpenAI', 'MIT'])} —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –±–ª–æ–≥
- –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ {random.choice(['Gartner', 'McKinsey'])}
"""
        # –í—Å—Ç–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ H2
        parts = re.split(r'(##\s+.*?)\n', content, maxsplit=1)
        if len(parts) >= 3:
            content = parts[0] + parts[1] + '\n' + concrete + '\n' + parts[2]
        else:
            content += concrete
    
    return content

# ==================== –ì–ï–ù–ï–†–ê–¶–ò–Ø –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø ====================

def generate_image(client: Groq, title: str, trend: Dict, post_num: int) -> bool:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"""
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç
    img_styles = [
        "Technical infographic with data charts and metrics, professional design, 16:9",
        "Isometric tech illustration with labels, clean blue-orange palette, 16:9",
        "Realistic 3D render of AI hardware, detailed, cinematic lighting, 16:9",
        "Conceptual architecture diagram with arrows, modern, high contrast, 16:9",
        "Data visualization chart, scientific style, labeled axes, 16:9"
    ]
    
    prompt = f"""
Technical illustration: {' '.join(trend['keywords'][:2])}
Style: {random.choice(img_styles)}
Include: {trend['news'][:100]}..., data labels, numbers
Format: PNG, 1280x720, professional
"""
    
    # –ü—ã—Ç–∞–µ–º—Å—è —É–ª—É—á—à–∏—Ç—å —á–µ—Ä–µ–∑ Groq
    try:
        improved = client.chat.completions.create(
            messages=[
                {"role": "system", "content": "–°–æ–∑–¥–∞–≤–∞–π –¥–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–π –ò–ò"},
                {"role": "user", "content": f"–£–ª—É—á—à–∏: {prompt}"}
            ],
            model="llama-3.1-8b-instant",
            max_tokens=100
        ).choices[0].message.content.strip()
    except:
        improved = prompt
    
    # Clipdrop API
    clipdrop_key = os.getenv("CLIPDROP_API_KEY")
    if clipdrop_key:
        try:
            response = requests.post(
                "https://clipdrop-api.co/text-to-image/v1",
                files={'prompt': (None, improved)},
                headers={'x-api-key': clipdrop_key},
                timeout=30
            )
            
            if response.status_code == 200:
                image_path = f"{assets_dir}/post-{post_num}.png"
                with open(image_path, "wb") as f:
                    f.write(response.content)
                print(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}")
                return True
        except Exception as e:
            print(f"‚ùå Clipdrop –æ—à–∏–±–∫–∞: {e}")
    
    # Fallback - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≥—Ä–∞—Ñ–∏–∫
    return generate_fallback_chart(post_num)

def generate_fallback_chart(post_num: int) -> bool:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥—Ä–∞—Ñ–∏–∫ —á–µ—Ä–µ–∑ matplotlib –∫–∞–∫ fallback"""
    try:
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt
        
        # –î–∞–Ω–Ω—ã–µ –∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
        categories = ['2023', '2024', '2025']
        values = [random.randint(50, 100), random.randint(100, 200), random.randint(200, 350)]
        
        plt.figure(figsize=(12, 6))
        plt.plot(categories, values, marker='o', linewidth=3, markersize=8)
        plt.title('AI Adoption Growth 2023-2025', fontsize=14, fontweight='bold')
        plt.ylabel('Enterprise Adoption %')
        plt.grid(True, alpha=0.3)
        
        chart_path = f"{assets_dir}/post-{post_num}.png"
        plt.savefig(chart_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫: {chart_path}")
        return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥—Ä–∞—Ñ–∏–∫–∞: {e}")
        return False

# ==================== –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ====================

def main():
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π)"""
    print(f"\n{'='*60}")
    print(f"ü§ñ AI Blog Generator | {datetime.datetime.now()}")
    print(f"{'='*60}\n")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º API –∫–ª—é—á
    groq_key = os.getenv("GROQ_API_KEY")
    if not groq_key:
        print("‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: GROQ_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return False
    
    client = Groq(api_key=groq_key)
    
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç—Ä–µ–Ω–¥—ã (—Å –∞–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º)
    trends = load_trends()
    if not trends:
        print("‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤")
        return False
    
    # 2. –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —Ç—Ä–µ–Ω–¥
    trend = random.choice(trends)
    print(f"üìà –¢—Ä–µ–Ω–¥: {trend['keywords'][0]} ({trend['category']})")
    print(f"üìù –ù–æ–≤–æ—Å—Ç—å: {trend['news'][:80]}...")
    
    # 3. –í—ã–±–∏—Ä–∞–µ–º —Ç–∏–ø —Å—Ç–∞—Ç—å–∏
    article_type = random.choice(["–û–±–∑–æ—Ä", "–£—Ä–æ–∫", "–°—Ç–∞—Ç—å—è", "–ú–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å"])
    print(f"üìö –¢–∏–ø —Å—Ç–∞—Ç—å–∏: {article_type}")
    
    # 4. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    title = generate_title(client, trend, article_type)
    if not title:
        title = f"{article_type} –ø—Ä–æ {trend['keywords'][0]}"
    print(f"üî• –ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}")
    
    # 5. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
    content = generate_article(client, trend, article_type)
    if not content:
        print("‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
        return False
    
    # 6. –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∏ —Ä–µ—Ñ–∞–π–Ω–∏–º
    content = validate_and_refine(content, trend)
    
    # 7. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    global post_num
    image_generated = generate_image(client, title, trend, post_num)
    
    # 8. –°–æ–∑–¥–∞–µ–º front matter
    front_matter = {
        "title": title,
        "date": f"{today} 00:00:00 -0000",
        "layout": "post",
        "image": f"/assets/images/posts/post-{post_num}.png",
        "image_alt": f"AI technologies 2025: {title[:60]}",
        "description": f"{article_type.lower()} –æ {trend['keywords'][0]} 2025: {content[:150]}...",
        "tags": ["–ò–ò", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", article_type.lower()] + trend['keywords'][:3],
        "keywords": json.dumps(trend['keywords'][:8]),
        "read_time": f"{max(5, len(content.split()) // 200)} –º–∏–Ω",
        "trend_id": trend["id"],
        "generated_at": datetime.datetime.now().isoformat()
    }
    
    # 9. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
    slug = re.sub(r'[^–∞-—è–ê-–Øa-zA-Z0-9-]', '-', title.lower().replace(" ", "-"))[:50]
    filename = f"{posts_dir}/{today}-{slug}.md"
    
    try:
        with open(filename, "w", encoding="utf-8") as f:
            f.write("---\n")
            yaml.dump(front_matter, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
            f.write("---\n\n")
            f.write(content)
        
        print(f"\n‚úÖ –£—Å–ø–µ—Ö! {filename}")
        print(f"   –†–∞–∑–º–µ—Ä: {len(content)//1024}KB | –°–ª–æ–≤: {len(content.split())}")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
        return False

# ==================== –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ====================

if __name__ == "__main__":
    # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    posts_dir = '_posts'
    assets_dir = 'assets/images/posts'
    os.makedirs(assets_dir, exist_ok=True)
    os.makedirs(posts_dir, exist_ok=True)
    
    # –ù—É–º–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    image_files = glob.glob(f"{assets_dir}/*.png") + glob.glob(f"{assets_dir}/*.jpg")
    post_num = len(image_files) + 1
    
    # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –ø–æ—Å—Ç–æ–≤ (–æ—Å—Ç–∞–≤–ª—è–µ–º 50 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö)
    post_files = sorted(glob.glob(f"{posts_dir}/*.md"), key=os.path.getctime, reverse=True)
    for old_file in post_files[50:]:
        try:
            os.remove(old_file)
            print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π –ø–æ—Å—Ç: {old_file}")
        except:
            pass
    
    # –¢–∏–ø—ã —Å—Ç–∞—Ç–µ–π
    types = ["–û–±–∑–æ—Ä", "–£—Ä–æ–∫", "–°—Ç–∞—Ç—å—è", "–ú–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å"]
    today = datetime.date.today()
    
    # –ó–∞–ø—É—Å–∫
    success = main()
    exit(0 if success else 1)
