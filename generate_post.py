#!/usr/bin/env python3
"""
–ê–≤—Ç–æ–Ω–æ–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç–∞—Ç–µ–π –æ–± –ò–ò 2025-2026
- –°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Ç—Ä–µ–Ω–¥—ã
- –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä—É—Å—Å–∫—É—é —Å—Ç–∞—Ç—å—é —Å —Ü–∏—Ñ—Ä–∞–º–∏ –∏ —Ç–∞–±–ª–∏—Ü–∞–º–∏
- –°–æ–∑–¥–∞–µ—Ç —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É –∏ —Ç–∏–∑–µ—Ä—É (–∞–Ω–≥–ª. –ø—Ä–æ–º–ø—Ç)
- –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ç–∏–∑–µ—Ä –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ Telegram
- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è GitHub Actions
"""

import datetime
import random
import os
import re
import json
import time
import glob
from typing import Dict, List, Optional

import requests
import yaml
from groq import Groq

# ---------- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ----------
EMBEDDED_TRENDS_FILE = "trends_cache.json"
TRENDS_UPDATE_INTERVAL = 86400  # 24 —á–∞—Å–∞
BASE_URL = "https://lybra-ai.ru"

# ---------- MARKDOWN NORMALIZER ----------
def normalize_markdown(md: str) -> str:
    """–ü—Ä–∏–≤–æ–¥–∏—Ç –≤—ã–≤–æ–¥ LLM –∫ –≤–∞–ª–∏–¥–Ω–æ–º—É Markdown (Jekyll-safe), –Ω–µ –º–µ–Ω—è—è —Å–º—ã—Å–ª–∞"""
    if not md:
        return md

    md = re.sub(r"<[^>]+>", "", md)
    md = re.sub(r"(#+\s.*)", r"\n\1\n", md)
    md = re.sub(r"\n([*-]\s)", r"\n\n\1", md)
    md = re.sub(r"\n(\|.*\|)\n(\|[-: ]+\|)", r"\n\n\1\n\2", md)
    md = re.sub(r"\n{3,}", "\n\n", md)

    return md.strip() + "\n"

# ---------- TRENDS (FALLBACK) ----------
EMBEDDED_TRENDS = [
    {"id": "quantum_2025", "news": "Google Willow quantum chip achieves verifiable quantum advantage, 13000x faster", "keywords": ["quantum computing", "Google Willow"], "category": "hardware"},
    {"id": "m5_chip_2025", "news": "Apple M5 delivers 4x GPU performance for AI vs M4, Nvidia DGX Spark 1 petaflop", "keywords": ["Apple M5", "Nvidia DGX"], "category": "hardware"},
    {"id": "agentic_ai_2025", "news": "Multi-agent systems and Agentic AI integrate RAG for enterprise", "keywords": ["Agentic AI", "RAG"], "category": "software"},
    {"id": "medical_ai_2025", "news": "BInD model designs drugs without molecular data, FDA approved 223 AI devices", "keywords": ["AI drug discovery", "FDA"], "category": "healthcare"},
    {"id": "efficiency_2025", "news": "GPT-3.5 inference cost dropped 280x in 2 years, open-weights closed gap 1.7%", "keywords": ["model efficiency", "open weights"], "category": "optimization"},
]

# ---------- TRENDS SYSTEM ----------
def load_trends() -> List[Dict]:
    try:
        if os.path.exists(EMBEDDED_TRENDS_FILE):
            with open(EMBEDDED_TRENDS_FILE, "r", encoding="utf-8") as f:
                cache = json.load(f)
                if time.time() - cache.get("last_update", 0) < TRENDS_UPDATE_INTERVAL:
                    print("‚úÖ –¢—Ä–µ–Ω–¥—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞")
                    return cache.get("trends", [])
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∫—ç—à–∞: {e}")
    return update_trends_cache()

def update_trends_cache() -> List[Dict]:
    print("üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–æ–≤...")
    trends: List[Dict] = []

    api_key = os.getenv("NEWSAPI_KEY")
    if api_key:
        try:
            resp = requests.get(
                "https://newsapi.org/v2/everything",
                headers={"X-Api-Key": api_key},
                params={"q": "artificial intelligence", "language": "en", "pageSize": 10},
                timeout=10)
            if resp.status_code == 200:
                data = resp.json()
                for i, a in enumerate(data.get("articles", [])[:10]):
                    title = a.get("title", "")
                    trends.append({
                        "id": f"news_{i}_{int(time.time())}",
                        "news": title + ". " + (a.get("description") or ""),
                        "keywords": title.lower().split()[:5],
                        "category": "news"
                    })
        except Exception as e:
            print(f"‚ùå NewsAPI: {e}")

    if not trends:
        try:
            import feedparser
            feeds = [
                "https://www.artificialintelligence-news.com/feed/",
                "https://venturebeat.com/ai/feed/"
            ]
            ts = int(time.time())
            for url in feeds:
                feed = feedparser.parse(url)
                for i, e in enumerate(feed.entries[:5]):
                    title = e.get("title", "")
                    trends.append({
                        "id": f"rss_{i}_{ts}",
                        "news": title + ". " + e.get("description", "")[:200],
                        "keywords": title.lower().split()[:5],
                        "category": "rss"
                    })
        except Exception as e:
            print(f"‚ùå RSS: {e}")

    if not trends:
        print("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã")
        trends = EMBEDDED_TRENDS

    try:
        with open(EMBEDDED_TRENDS_FILE, "w", encoding="utf-8") as f:
            json.dump({"last_update": int(time.time()), "trends": trends}, f)
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à: {e}")
    return trends

# ---------- –ó–ê–ì–û–õ–û–í–û–ö ----------
def generate_title(client: Groq, trend: Dict, article_type: str) -> str:
    prompt = (
        f"–°–æ–∑–¥–∞–π –û–î–ò–ù —Ü–µ–ø–ª—è—é—â–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ (5‚Äì12 —Å–ª–æ–≤) –¥–ª—è —Å—Ç–∞—Ç—å–∏ —Ç–∏–ø–∞ '{article_type}'.\n"
        f"–¢–µ–º–∞: {trend['news']}.\n"
        "–°—Ç–∏–ª—å: –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ, –ø–æ–ª–µ–∑–Ω–æ, –∏–Ω—Ç—Ä–∏–≥—É—é—â–µ, —Å —Ü–∏—Ñ—Ä–∞–º–∏ –∏–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º.\n"
        "–ó–∞–ø—Ä–µ—â–µ–Ω–æ: –ø–æ–ª–∏—Ç–∏–∫–∞, —Å—Ç—Ä–∞–Ω—ã, —Ä–µ–≥—É–ª—è—Ç–æ—Ä—ã, –∑–∞–∫–æ–Ω—ã, —É–∫–∞–∑—ã, –ª–∏–¥–µ—Ä—ã.\n"
        "–†–∞–∑—Ä–µ—à–µ–Ω–æ: –ò–ò, —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –ø—Ä–æ–¥—É–∫—Ç—ã, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, —Ä—ã–Ω–æ–∫, –º–µ—Ç—Ä–∏–∫–∏.\n"
        "–¢–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫, –±–µ–∑ –∫–∞–≤—ã—á–µ–∫."
    )

    resp = client.chat.completions.create(
        messages=[
            {"role": "system", "content": "–†—É—Å—Å–∫–∏–π —Ç–µ—Ö-—Ä–µ–¥–∞–∫—Ç–æ—Ä. –î–µ–ª–∞–π –∑–∞–≥–æ–ª–æ–≤–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ö–æ—á–µ—Ç—Å—è –æ—Ç–∫—Ä—ã—Ç—å."},
            {"role": "user", "content": prompt},
        ],
        model="llama-3.1-8b-instant",
        max_tokens=30,
        temperature=1.0
    )
    title = resp.choices[0].message.content.strip()
    return re.sub(r"[^\w\s-]", "", title)[:80]

# ---------- –°–¢–ê–¢–¨–Ø ----------
def generate_article(client: Groq, trend: Dict, article_type: str) -> str:
    system_prompt = f"""
–í—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∂—É—Ä–Ω–∞–ª–∏—Å—Ç –ø–æ –ò–ò –∏ –≤—ã—Å–æ–∫–∏–º —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º.
–ê—É–¥–∏—Ç–æ—Ä–∏—è: —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏, –∏–Ω–∂–µ–Ω–µ—Ä—ã, —Ñ–∞—É–Ω–¥–µ—Ä—ã, —Ç–µ—Ö-—ç–Ω—Ç—É–∑–∏–∞—Å—Ç—ã.

–§–æ–∫—É—Å:
- –∫–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç
- –∑–∞—á–µ–º —ç—Ç–æ –Ω—É–∂–Ω–æ
- —Ü–∏—Ñ—Ä—ã, —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, —Ä–µ–∞–ª—å–Ω—ã–µ –∫–µ–π—Å—ã
- –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –ø–æ–ª—å–∑–∞

–°–¢–†–û–ì–û –ó–ê–ü–†–ï–©–ï–ù–û:
–ø–æ–ª–∏—Ç–∏–∫–∞, —Å—Ç—Ä–∞–Ω—ã, —Ä–µ–≥—É–ª—è—Ç–æ—Ä—ã, –∑–∞–∫–æ–Ω—ã, —É–∫–∞–∑—ã, –ª–∏–¥–µ—Ä—ã, –≤–µ–¥–æ–º—Å—Ç–≤–∞.

–§–æ—Ä–º–∞—Ç:
- Markdown
- ## –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∏
- —Å–ø–∏—Å–∫–∏
- —Ç–∞–±–ª–∏—Ü—ã
- –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

–¢–µ–º–∞: {trend['news']}
"""

    user_prompt = (
        f"–ù–∞–ø–∏—à–∏—Ç–µ –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç—å—é —Ç–∏–ø–∞ '{article_type}' (1500‚Äì3000 —Å–ª–æ–≤).\n"
        "–î–æ–±–∞–≤—å—Ç–µ:\n"
        "- –º–∏–Ω–∏–º—É–º 2 —Ç–∞–±–ª–∏—Ü—ã\n"
        "- —Ä–µ–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n"
        "- –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã\n"
        "- –≤—ã–≤–æ–¥—ã –∏ –ø—Ä–æ–≥–Ω–æ–∑—ã\n"
    )

    resp = client.chat.completions.create(
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        model="llama-3.3-70b-versatile",
        max_tokens=4000,
        temperature=0.85
    )
    return normalize_markdown(resp.choices[0].message.content)

# ---------- –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ï ----------
def generate_image(client: Groq, title: str, trend: Dict, post_num: int) -> bool:
    teaser = trend["news"][:120]

    prompt = (
        f"Ultra-realistic photo illustration of: {title}. {teaser}. "
        "Scene-based, cinematic, real-world environment, modern technology.\n"
        "NO charts, NO graphs, NO diagrams, NO infographics, NO plots, NO text overlays.\n"
        "NO politics, NO flags, NO leaders.\n"
        "Professional photography, shallow depth of field, 8K, photorealistic."
    )

    clipdrop_key = os.getenv("CLIPDROP_API_KEY")
    if clipdrop_key:
        try:
            resp = requests.post(
                "https://clipdrop-api.co/text-to-image/v1",
                files={"prompt": (None, prompt)},
                headers={"x-api-key": clipdrop_key},
                timeout=90
            )
            if resp.status_code == 200:
                path = f"{assets_dir}/post-{post_num}.png"
                with open(path, "wb") as f:
                    f.write(resp.content)

                try:
                    from PIL import Image
                    img = Image.open(path)
                    img.save(path, optimize=True)
                except Exception:
                    pass

                return True
        except Exception:
            pass

    return generate_fallback_chart(post_num)

def generate_fallback_chart(post_num: int) -> bool:
    try:
        import matplotlib
        matplotlib.use("Agg")
        import matplotlib.pyplot as plt

        years = ["2023", "2024", "2025"]
        values = [random.randint(40, 100), random.randint(100, 200), random.randint(200, 350)]

        plt.figure(figsize=(12, 6))
        plt.plot(years, values, marker="o", linewidth=3)
        plt.title("AI Trend Growth")
        plt.tight_layout()
        plt.savefig(f"{assets_dir}/post-{post_num}.png", dpi=150)
        plt.close()
        return True
    except Exception:
        return False

# ---------- MAIN ----------
def main() -> bool:
    client = Groq(api_key=os.getenv("GROQ_API_KEY"))
    trends = load_trends()
    if not trends:
        return False

    trend = random.choice(trends)
    article_type = random.choice(["–û–±–∑–æ—Ä", "–£—Ä–æ–∫", "–°—Ç–∞—Ç—å—è", "–ú–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å"])

    title = generate_title(client, trend, article_type)
    content = generate_article(client, trend, article_type)
    generate_image(client, title, trend, post_num)

    front_matter = {
        "title": title,
        "date": f"{today} 00:00:00 +0000",
        "layout": "post",
        "image": f"/assets/images/posts/post-{post_num}.png",
        "description": f"{article_type.lower()} –æ {trend['keywords'][0]} 2025",
        "tags": ["–ò–ò", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", article_type.lower()] + trend["keywords"],
    }

    slug = re.sub(r"[^\w-]", "-", title.lower())
    slug = re.sub(r"-{2,}", "-", slug).strip("-")[:50]
    filename = f"{posts_dir}/{today}-{slug}.md"

    with open(filename, "w", encoding="utf-8") as f:
        f.write("---\n")
        yaml.dump(front_matter, f, allow_unicode=True, sort_keys=False)
        f.write("---\n\n")
        f.write(content)

    return True

# ---------- INIT ----------
if __name__ == "__main__":
    posts_dir = "_posts"
    assets_dir = "assets/images/posts"
    os.makedirs(posts_dir, exist_ok=True)
    os.makedirs(assets_dir, exist_ok=True)

    image_files = glob.glob(f"{assets_dir}/*.png") + glob.glob(f"{assets_dir}/*.jpg")
    post_num = len(image_files) + 1

    for old_file in sorted(
        glob.glob(f"{posts_dir}/*.md"),
        key=os.path.getctime,
        reverse=True
    )[50:]:
        try:
            os.remove(old_file)
        except Exception:
            pass

    today = datetime.date.today()
    success = main()
    raise SystemExit(0 if success else 1)
